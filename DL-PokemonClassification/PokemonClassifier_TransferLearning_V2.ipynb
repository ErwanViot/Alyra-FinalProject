{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Pokemon Classifier - Transfer Learning (EfficientNetB0)\n",
    "\n",
    "Classification de 151 Pokémon avec Transfer Learning.\n",
    "\n",
    "**Stratégie**: Feature Extraction (30 epochs) puis Fine-Tuning (80 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom pathlib import Path\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\nprint(f\"TensorFlow: {tf.__version__}\")\nprint(f\"GPU: {tf.config.list_physical_devices('GPU')}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('./PokemonData')\n",
    "image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp'}\n",
    "\n",
    "pokemon_dirs = sorted([d for d in data_path.iterdir() if d.is_dir()])\n",
    "image_paths, labels = [], []\n",
    "\n",
    "for pokemon_dir in pokemon_dirs:\n",
    "    for img in pokemon_dir.iterdir():\n",
    "        if img.suffix.lower() in image_extensions:\n",
    "            image_paths.append(str(img))\n",
    "            labels.append(pokemon_dir.name)\n",
    "\n",
    "image_paths = np.array(image_paths)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f\"Images: {len(image_paths)} | Classes: {len(np.unique(labels))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Split Train/Val/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
    "    image_paths, labels, test_size=0.2, stratify=labels, random_state=42)\n",
    "\n",
    "val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
    "    temp_paths, temp_labels, test_size=0.5, stratify=temp_labels, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train_paths)} | Val: {len(val_paths)} | Test: {len(test_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = sorted(np.unique(train_labels))\n",
    "label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "train_labels_onehot = tf.keras.utils.to_categorical([label_to_idx[l] for l in train_labels], num_classes)\n",
    "val_labels_onehot = tf.keras.utils.to_categorical([label_to_idx[l] for l in val_labels], num_classes)\n",
    "test_labels_onehot = tf.keras.utils.to_categorical([label_to_idx[l] for l in test_labels], num_classes)\n",
    "\n",
    "def load_image(path, label):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "    img = tf.image.resize(img, (256, 256))\n",
    "    return img, label\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.15),\n",
    "    layers.RandomZoom(0.1),\n",
    "    layers.RandomContrast(0.15),\n",
    "    layers.RandomBrightness(0.15),\n",
    "])\n",
    "\n",
    "def augment_and_preprocess(img, label):\n",
    "    img = data_augmentation(img, training=True)\n",
    "    return preprocess_input(img), label\n",
    "\n",
    "def preprocess_only(img, label):\n",
    "    return preprocess_input(img), label\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = (tf.data.Dataset.from_tensor_slices((train_paths, train_labels_onehot))\n",
    "    .map(load_image, num_parallel_calls=AUTOTUNE)\n",
    "    .map(augment_and_preprocess, num_parallel_calls=AUTOTUNE)\n",
    "    .shuffle(1000).batch(batch_size).prefetch(AUTOTUNE))\n",
    "\n",
    "val_dataset = (tf.data.Dataset.from_tensor_slices((val_paths, val_labels_onehot))\n",
    "    .map(load_image, num_parallel_calls=AUTOTUNE)\n",
    "    .map(preprocess_only, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(batch_size).prefetch(AUTOTUNE))\n",
    "\n",
    "test_dataset = (tf.data.Dataset.from_tensor_slices((test_paths, test_labels_onehot))\n",
    "    .map(load_image, num_parallel_calls=AUTOTUNE)\n",
    "    .map(preprocess_only, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(batch_size).prefetch(AUTOTUNE))\n",
    "\n",
    "print(\"Datasets prêts avec preprocess_input EfficientNet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Architecture Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = EfficientNetB0(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(256, 256, 3)\n",
    ")\n",
    "base_model.trainable = False\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "print(f\"\\nBase model: {base_model.count_params():,} params (frozen)\")\n",
    "print(f\"Trainable: {sum([tf.keras.backend.count_params(w) for w in model.trainable_weights]):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Phase 1: Feature Extraction (30 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PHASE 1: Feature Extraction\")\n",
    "print(\"Base model FROZEN | LR: 1e-3 | Epochs: 30\")\n",
    "\n",
    "callbacks_p1 = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7)\n",
    "]\n",
    "\n",
    "history_p1 = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=30,\n",
    "    callbacks=callbacks_p1\n",
    ")\n",
    "\n",
    "print(f\"\\nPhase 1 terminée - Best val acc: {max(history_p1.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Phase 2: Fine-Tuning (80 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PHASE 2: Fine-Tuning\")\n",
    "\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-30]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "trainable = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "print(f\"30 dernières couches dégelées | LR: 1e-5 | Epochs: 80\")\n",
    "print(f\"Paramètres trainables: {trainable:,}\")\n",
    "\n",
    "callbacks_p2 = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7)\n",
    "]\n",
    "\n",
    "history_p2 = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=80,\n",
    "    callbacks=callbacks_p2\n",
    ")\n",
    "\n",
    "print(f\"\\nPhase 2 terminée - Best val acc: {max(history_p2.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Courbes d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Phase 1\n",
    "axes[0, 0].plot(history_p1.history['loss'], label='Train')\n",
    "axes[0, 0].plot(history_p1.history['val_loss'], label='Val')\n",
    "axes[0, 0].set_title('Phase 1 - Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(history_p1.history['accuracy'], label='Train')\n",
    "axes[0, 1].plot(history_p1.history['val_accuracy'], label='Val')\n",
    "axes[0, 1].set_title('Phase 1 - Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Phase 2\n",
    "axes[1, 0].plot(history_p2.history['loss'], label='Train')\n",
    "axes[1, 0].plot(history_p2.history['val_loss'], label='Val')\n",
    "axes[1, 0].set_title('Phase 2 - Loss')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(history_p2.history['accuracy'], label='Train')\n",
    "axes[1, 1].plot(history_p2.history['val_accuracy'], label='Val')\n",
    "axes[1, 1].set_title('Phase 2 - Accuracy')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "p1_best = max(history_p1.history['val_accuracy'])\n",
    "p2_best = max(history_p2.history['val_accuracy'])\n",
    "print(f\"Phase 1: {p1_best:.4f} | Phase 2: {p2_best:.4f} | Gain: +{(p2_best-p1_best)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Sauvegarde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('pokemon_classifier_efficientnet.keras')\n",
    "print(f\"Modèle sauvegardé: pokemon_classifier_efficientnet.keras\")\n",
    "print(f\"Taille: {Path('pokemon_classifier_efficientnet.keras').stat().st_size / (1024*1024):.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Évaluation sur le Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Matrice de confusion - Top 20 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict(test_dataset)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "y_true = np.argmax(test_labels_onehot, axis=1)\n",
    "\n",
    "top_20_classes = pd.Series(y_true).value_counts().head(20).index.tolist()\n",
    "top_20_names = [list(label_to_idx.keys())[i] for i in top_20_classes]\n",
    "\n",
    "mask = np.isin(y_true, top_20_classes)\n",
    "cm = confusion_matrix(y_true[mask], y_pred[mask], labels=top_20_classes)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=top_20_names, yticklabels=top_20_names)\n",
    "plt.title('Confusion Matrix - Top 20 classes')\n",
    "plt.xlabel('Prédiction')\n",
    "plt.ylabel('Vérité')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Matrice de confusion - Worst 20 classes\n",
    "\n",
    "Classes avec le plus d'erreurs de classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_true, y_pred, target_names=list(label_to_idx.keys()), \n",
    "                               output_dict=True, zero_division=0)\n",
    "\n",
    "class_f1 = {name: report[name]['f1-score'] for name in label_to_idx.keys()}\n",
    "worst_20_names = sorted(class_f1, key=class_f1.get)[:20]\n",
    "worst_20_classes = [label_to_idx[name] for name in worst_20_names]\n",
    "\n",
    "mask_worst = np.isin(y_true, worst_20_classes)\n",
    "cm_worst = confusion_matrix(y_true[mask_worst], y_pred[mask_worst], labels=worst_20_classes)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm_worst, annot=True, fmt='d', cmap='Reds', \n",
    "            xticklabels=worst_20_names, yticklabels=worst_20_names)\n",
    "plt.title('Confusion Matrix - Worst 20 classes (plus faibles F1-scores)')\n",
    "plt.xlabel('Prédiction')\n",
    "plt.ylabel('Vérité')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Classes avec les plus faibles performances:\")\n",
    "for name in worst_20_names[:10]:\n",
    "    print(f\"  {name}: F1={class_f1[name]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Métriques globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy:  {report['accuracy']:.4f}\")\n",
    "print(f\"Precision: {report['macro avg']['precision']:.4f}\")\n",
    "print(f\"Recall:    {report['macro avg']['recall']:.4f}\")\n",
    "print(f\"F1-Score:  {report['macro avg']['f1-score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## Visualisation des prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random_indices = np.random.choice(len(test_paths), 12, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(14, 10))\n",
    "\n",
    "for idx, test_idx in enumerate(random_indices):\n",
    "    img = tf.io.read_file(test_paths[test_idx])\n",
    "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "    img_resized = tf.image.resize(img, (256, 256))\n",
    "    \n",
    "    prediction = model.predict(tf.expand_dims(preprocess_input(img_resized), 0), verbose=0)\n",
    "    predicted_class = list(label_to_idx.keys())[np.argmax(prediction[0])]\n",
    "    confidence = np.max(prediction[0])\n",
    "    true_class = test_labels[test_idx]\n",
    "    \n",
    "    axes.flat[idx].imshow(img_resized.numpy().astype('uint8'))\n",
    "    color = 'green' if predicted_class == true_class else 'red'\n",
    "    axes.flat[idx].set_title(f\"{true_class}\\n-> {predicted_class} ({confidence*100:.0f}%)\", \n",
    "                              fontsize=8, color=color)\n",
    "    axes.flat[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## Test sur images externes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_dir = Path('./Test_Img_Externes')\n",
    "\n",
    "if external_dir.exists():\n",
    "    external_images = [f for f in external_dir.iterdir() \n",
    "                       if f.suffix.lower() in image_extensions]\n",
    "    \n",
    "    if external_images:\n",
    "        num_images = min(len(external_images), 8)\n",
    "        fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
    "        \n",
    "        for idx, img_path in enumerate(external_images[:num_images]):\n",
    "            img = tf.io.read_file(str(img_path))\n",
    "            img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "            img_resized = tf.image.resize(img, (256, 256))\n",
    "            \n",
    "            prediction = model.predict(tf.expand_dims(preprocess_input(img_resized), 0), verbose=0)\n",
    "            top3_idx = np.argsort(prediction[0])[-3:][::-1]\n",
    "            \n",
    "            axes.flat[idx].imshow(img_resized.numpy().astype('uint8'))\n",
    "            title = \"\\n\".join([f\"{list(label_to_idx.keys())[i]} ({prediction[0][i]*100:.0f}%)\" \n",
    "                               for i in top3_idx])\n",
    "            axes.flat[idx].set_title(title, fontsize=7)\n",
    "            axes.flat[idx].axis('off')\n",
    "        \n",
    "        for idx in range(num_images, 8):\n",
    "            axes.flat[idx].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(f\"Répertoire {external_dir} non trouvé\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}